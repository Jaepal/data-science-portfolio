{"paragraphs":[{"text":"import org.apache.spark.ml.recommendation.ALS\n\nval ratings = spark.read.option(\"header\", true).\n    option(\"inferSchema\", true).\n    csv(\"file:///Users/Lee/Desktop/dataset/anime-recommendations-database/rating.csv\")\nval anime = spark.read.option(\"header\", true).\n    option(\"inferSchema\", true).\n    csv(\"file:///Users/Lee/Desktop/dataset/anime-recommendations-database/anime.csv\")\nratings.show(false)\nanime.show(false)","user":"anonymous","dateUpdated":"2019-09-23T19:38:05+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.recommendation.ALS\nratings: org.apache.spark.sql.DataFrame = [user_id: int, anime_id: int ... 1 more field]\nanime: org.apache.spark.sql.DataFrame = [anime_id: int, name: string ... 5 more fields]\n+-------+--------+------+\n|user_id|anime_id|rating|\n+-------+--------+------+\n|1      |20      |-1    |\n|1      |24      |-1    |\n|1      |79      |-1    |\n|1      |226     |-1    |\n|1      |241     |-1    |\n|1      |355     |-1    |\n|1      |356     |-1    |\n|1      |442     |-1    |\n|1      |487     |-1    |\n|1      |846     |-1    |\n|1      |936     |-1    |\n|1      |1546    |-1    |\n|1      |1692    |-1    |\n|1      |1836    |-1    |\n|1      |2001    |-1    |\n|1      |2025    |-1    |\n|1      |2144    |-1    |\n|1      |2787    |-1    |\n|1      |2993    |-1    |\n|1      |3455    |-1    |\n+-------+--------+------+\nonly showing top 20 rows\n\r\n+--------+---------------------------------------------------------+----------------------------------------------------------------------------+-----+--------+------+-------+\n|anime_id|name                                                     |genre                                                                       |type |episodes|rating|members|\n+--------+---------------------------------------------------------+----------------------------------------------------------------------------+-----+--------+------+-------+\n|32281   |Kimi no Na wa.                                           |Drama, Romance, School, Supernatural                                        |Movie|1       |9.37  |200630 |\n|5114    |Fullmetal Alchemist: Brotherhood                         |Action, Adventure, Drama, Fantasy, Magic, Military, Shounen                 |TV   |64      |9.26  |793665 |\n|28977   |Gintama°                                                 |Action, Comedy, Historical, Parody, Samurai, Sci-Fi, Shounen                |TV   |51      |9.25  |114262 |\n|9253    |Steins;Gate                                              |Sci-Fi, Thriller                                                            |TV   |24      |9.17  |673572 |\n|9969    |Gintama&#039;                                            |Action, Comedy, Historical, Parody, Samurai, Sci-Fi, Shounen                |TV   |51      |9.16  |151266 |\n|32935   |Haikyuu!!: Karasuno Koukou VS Shiratorizawa Gakuen Koukou|Comedy, Drama, School, Shounen, Sports                                      |TV   |10      |9.15  |93351  |\n|11061   |Hunter x Hunter (2011)                                   |Action, Adventure, Shounen, Super Power                                     |TV   |148     |9.13  |425855 |\n|820     |Ginga Eiyuu Densetsu                                     |Drama, Military, Sci-Fi, Space                                              |OVA  |110     |9.11  |80679  |\n|15335   |Gintama Movie: Kanketsu-hen - Yorozuya yo Eien Nare      |Action, Comedy, Historical, Parody, Samurai, Sci-Fi, Shounen                |Movie|1       |9.1   |72534  |\n|15417   |Gintama&#039;: Enchousen                                 |Action, Comedy, Historical, Parody, Samurai, Sci-Fi, Shounen                |TV   |13      |9.11  |81109  |\n|4181    |Clannad: After Story                                     |Drama, Fantasy, Romance, Slice of Life, Supernatural                        |TV   |24      |9.06  |456749 |\n|28851   |Koe no Katachi                                           |Drama, School, Shounen                                                      |Movie|1       |9.05  |102733 |\n|918     |Gintama                                                  |Action, Comedy, Historical, Parody, Samurai, Sci-Fi, Shounen                |TV   |201     |9.04  |336376 |\n|2904    |Code Geass: Hangyaku no Lelouch R2                       |Action, Drama, Mecha, Military, Sci-Fi, Super Power                         |TV   |25      |8.98  |572888 |\n|28891   |Haikyuu!! Second Season                                  |Comedy, Drama, School, Shounen, Sports                                      |TV   |25      |8.93  |179342 |\n|199     |Sen to Chihiro no Kamikakushi                            |Adventure, Drama, Supernatural                                              |Movie|1       |8.93  |466254 |\n|23273   |Shigatsu wa Kimi no Uso                                  |Drama, Music, Romance, School, Shounen                                      |TV   |22      |8.92  |416397 |\n|24701   |Mushishi Zoku Shou 2nd Season                            |Adventure, Fantasy, Historical, Mystery, Seinen, Slice of Life, Supernatural|TV   |10      |8.88  |75894  |\n|12355   |Ookami Kodomo no Ame to Yuki                             |Fantasy, Slice of Life                                                      |Movie|1       |8.84  |226193 |\n|1575    |Code Geass: Hangyaku no Lelouch                          |Action, Mecha, Military, School, Sci-Fi, Super Power                        |TV   |25      |8.83  |715151 |\n+--------+---------------------------------------------------------+----------------------------------------------------------------------------+-----+--------+------+-------+\nonly showing top 20 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1568341448616_203574865","id":"20190913-112408_920695659","dateCreated":"2019-09-13T11:24:08+0900","dateStarted":"2019-09-23T19:38:05+0900","dateFinished":"2019-09-23T19:38:31+0900","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:472"},{"text":"ratings.createOrReplaceTempView(\"ratings\")\nanime.createOrReplaceTempView(\"anime\")","user":"anonymous","dateUpdated":"2019-09-23T19:40:21+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1569235218215_-1868749561","id":"20190923-194018_445552788","dateCreated":"2019-09-23T19:40:18+0900","dateStarted":"2019-09-23T19:40:21+0900","dateFinished":"2019-09-23T19:40:22+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:473"},{"text":"val ratingsExplicit = spark.sql(\"\"\"\n    SELECT user_id, anime_id, rating\n    FROM ratings\n    WHERE rating != -1\n    \"\"\")\nratingsExplicit.show(false)","user":"anonymous","dateUpdated":"2019-09-23T19:41:16+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ratingsExplicit: org.apache.spark.sql.DataFrame = [user_id: int, anime_id: int ... 1 more field]\n+-------+--------+------+\n|user_id|anime_id|rating|\n+-------+--------+------+\n|1      |8074    |10    |\n|1      |11617   |10    |\n|1      |11757   |10    |\n|1      |15451   |10    |\n|2      |11771   |10    |\n|3      |20      |8     |\n|3      |154     |6     |\n|3      |170     |9     |\n|3      |199     |10    |\n|3      |225     |9     |\n|3      |341     |6     |\n|3      |430     |7     |\n|3      |527     |7     |\n|3      |552     |7     |\n|3      |813     |10    |\n|3      |1119    |7     |\n|3      |1121    |7     |\n|3      |1122    |7     |\n|3      |1132    |8     |\n|3      |1292    |6     |\n+-------+--------+------+\nonly showing top 20 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1569232468234_1205424860","id":"20190923-185428_735009844","dateCreated":"2019-09-23T18:54:28+0900","dateStarted":"2019-09-23T19:41:16+0900","dateFinished":"2019-09-23T19:41:19+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:474"},{"text":"val Array(training, test) = ratingsExplicit.randomSplit(Array(0.8, 0.2))\nval als = new ALS().\n    setMaxIter(5).\n    setRegParam(0.01).\n    setUserCol(\"user_id\").\n    setItemCol(\"anime_id\").\n    setRatingCol(\"rating\")\nprintln(als.explainParams)","user":"anonymous","dateUpdated":"2019-09-23T22:20:11+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, anime_id: int ... 1 more field]\r\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, anime_id: int ... 1 more field]\nals: org.apache.spark.ml.recommendation.ALS = als_146f23fcb91d\nalpha: alpha for implicit preference (default: 1.0)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations (default: 10)\ncoldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: nan,drop. (default: nan)\nfinalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\nimplicitPrefs: whether to use implicit preference (default: false)\nintermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\nitemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: anime_id)\nmaxIter: maximum number of iterations (>= 0) (default: 10, current: 5)\nnonnegative: whether to use nonnegative constraint for least squares (default: false)\nnumItemBlocks: number of item blocks (default: 10)\nnumUserBlocks: number of user blocks (default: 10)\npredictionCol: prediction column name (default: prediction)\nrank: rank of the factorization (default: 10)\nratingCol: column name for ratings (default: rating, current: rating)\nregParam: regularization parameter (>= 0) (default: 0.1, current: 0.01)\nseed: random seed (default: 1994790107)\nuserCol: column name for user ids. Ids must be within the integer value range. (default: user, current: user_id)\r\n"}]},"apps":[],"jobName":"paragraph_1568341798380_1497040543","id":"20190913-112958_1552668723","dateCreated":"2019-09-13T11:29:58+0900","dateStarted":"2019-09-23T22:20:11+0900","dateFinished":"2019-09-23T22:20:17+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:475"},{"text":"val alsModel = als.setColdStartStrategy(\"drop\").fit(training)\nval predictions = alsModel.transform(test)","user":"anonymous","dateUpdated":"2019-09-23T22:20:23+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"alsModel: org.apache.spark.ml.recommendation.ALSModel = als_146f23fcb91d\npredictions: org.apache.spark.sql.DataFrame = [user_id: int, anime_id: int ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1568341766053_393069378","id":"20190913-112926_975005230","dateCreated":"2019-09-13T11:29:26+0900","dateStarted":"2019-09-23T22:20:23+0900","dateFinished":"2019-09-23T22:21:43+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:476"},{"text":"alsModel.recommendForAllUsers(10).\n    selectExpr(\"user_id\", \"explode(recommendations)\").show()\nalsModel.recommendForAllItems(10).\n    selectExpr(\"anime_id\", \"explode(recommendations)\").show()","user":"anonymous","dateUpdated":"2019-09-23T23:17:17+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 161, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\Lee\\AppData\\Local\\Temp\\blockmgr-205c6ce3-cf50-438a-b963-cc711dcba443\\24\\temp_shuffle_194485e4-d54b-467d-8c93-ef114f10dc14 (지정된 경로를 찾을 수 없습니다)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at scala.Option.foreach(Option.scala:257)\r\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\r\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\r\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\r\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\r\n  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\r\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\r\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\r\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\r\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:644)\r\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:603)\r\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:612)\r\n  ... 52 elided\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Lee\\AppData\\Local\\Temp\\blockmgr-205c6ce3-cf50-438a-b963-cc711dcba443\\24\\temp_shuffle_194485e4-d54b-467d-8c93-ef114f10dc14 (지정된 경로를 찾을 수 없습니다)\r\n  at java.io.FileOutputStream.open0(Native Method)\r\n  at java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n  at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n  at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n  at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n  at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n  at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1568342627291_-633006445","id":"20190913-114347_450563178","dateCreated":"2019-09-13T11:43:47+0900","dateStarted":"2019-09-23T23:17:18+0900","dateFinished":"2019-09-23T23:17:29+0900","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:477"},{"text":"import org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval evaluator = new RegressionEvaluator().\n    setMetricName(\"rmse\").\n    setLabelCol(\"rating\").\n    setPredictionCol(\"prediction\")\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root-mean-square error = $rmse\")","user":"anonymous","dateUpdated":"2019-09-14T00:25:07+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.evaluation.RegressionEvaluator\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_0fcb5387b2a9\nrmse: Double = 1.1711753147013406\nRoot-mean-square error = 1.1711753147013406\r\n"}]},"apps":[],"jobName":"paragraph_1568344376205_-342907993","id":"20190913-121256_229413281","dateCreated":"2019-09-13T12:12:56+0900","dateStarted":"2019-09-14T00:25:07+0900","dateFinished":"2019-09-14T00:29:13+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:478"},{"text":"import org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}\nimport org.apache.spark.sql.functions.{col, expr}\n\nval perUserActual = predictions.\n    where(\"rating > 2.5\").\n    groupBy(\"user_id\").\n    agg(expr(\"collect_set(anime_id) as animes\"))","user":"anonymous","dateUpdated":"2019-09-14T01:22:05+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}\nimport org.apache.spark.sql.functions.{col, expr}\nperUserActual: org.apache.spark.sql.DataFrame = [user_id: int, animes: array<int>]\n"}]},"apps":[],"jobName":"paragraph_1568380837116_1967802695","id":"20190913-222037_544350806","dateCreated":"2019-09-13T22:20:37+0900","dateStarted":"2019-09-14T01:22:05+0900","dateFinished":"2019-09-14T01:22:07+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:479"},{"text":"val  perUserPredictions = predictions.\n    orderBy(col(\"user_id\"), col(\"prediction\").desc).\n    groupBy(\"user_id\").\n    agg(expr(\"collect_list(anime_id) as animes\"))","user":"anonymous","dateUpdated":"2019-09-14T01:24:04+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"perUserPredictions: org.apache.spark.sql.DataFrame = [user_id: int, animes: array<int>]\n"}]},"apps":[],"jobName":"paragraph_1568391700392_-1668155098","id":"20190914-012140_736484518","dateCreated":"2019-09-14T01:21:40+0900","dateStarted":"2019-09-14T01:24:04+0900","dateFinished":"2019-09-14T01:24:05+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:480"},{"text":"val perUserActualvPred = perUserActual.join(perUserPredictions, Seq(\"user_id\")).\n    map(row => (\n        row(1).asInstanceOf[Seq[Integer]].toArray,\n        row(2).asInstanceOf[Seq[Integer]].toArray.take(15)\n    ))\nval ranks = new RankingMetrics(perUserActualvPred.rdd)","user":"anonymous","dateUpdated":"2019-09-14T01:26:46+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"perUserActualvPred: org.apache.spark.sql.Dataset[(Array[Integer], Array[Integer])] = [_1: array<int>, _2: array<int>]\nranks: org.apache.spark.mllib.evaluation.RankingMetrics[Integer] = org.apache.spark.mllib.evaluation.RankingMetrics@454927cd\n"}]},"apps":[],"jobName":"paragraph_1568391844176_-111987396","id":"20190914-012404_1443014229","dateCreated":"2019-09-14T01:24:04+0900","dateStarted":"2019-09-14T01:26:46+0900","dateFinished":"2019-09-14T01:31:54+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:481"},{"text":"ranks.meanAveragePrecision\nranks.precisionAt(5)","user":"anonymous","dateUpdated":"2019-09-14T01:32:45+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res54: Double = 0.815350418960177\nres55: Double = 0.6514705882352941\n"}]},"apps":[],"jobName":"paragraph_1568391990359_-1312275200","id":"20190914-012630_651182197","dateCreated":"2019-09-14T01:26:30+0900","dateStarted":"2019-09-14T01:32:46+0900","dateFinished":"2019-09-14T01:47:29+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:482"},{"text":"import ","user":"anonymous","dateUpdated":"2019-09-14T22:27:02+0900","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1568392365848_-229102132","id":"20190914-013245_484083267","dateCreated":"2019-09-14T01:32:45+0900","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:483"}],"name":"animeRecommendation","id":"2EQ5GQTU2","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}